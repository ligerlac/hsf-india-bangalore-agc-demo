{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"06147016-2fd0-4f3f-8c41-18069986cd25","cell_type":"markdown","source":"# Introduction\n\nWe will be working on a $t\\bar{t}$ cross section measurement analysis using opendata from the CMS experiment. The lepton+jets final state is chosen: $t\\bar{t} \\to bW^+bW^- \\to bq\\bar{q}bl^-\\nu_l$, characterized by one lepton (here we look at electrons and muons only), significant missing transverse energy, and four jets, two of which are b-tagged. We will first start by inspecting one [root file](https://root.cern/manual/root_files/) from the simulated inclusive $t\\bar{t}$ sample. [Coffea](https://coffea-hep.readthedocs.io/en/stable/) (Columnar Object Framework for Effective Analysis) is a python package used for this purpose. It uses uproot for reading root files, awkward arrays to represent events as columnar information. To read the CMS NanoAOD format, we use `NanoEventsFactory` and `NanoAODSchema` but other methods exist to read flat ntuples as well.","metadata":{}},{"id":"c41e9007-0731-46d2-a087-c76388cc4491","cell_type":"code","source":"from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\nevents = NanoEventsFactory.from_root(\n{\"https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0000.root\" : \"Events\"},\n    schemaclass=NanoAODSchema,\n    entry_start=0,\n    entry_stop=1000,\n).events()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e4027c5a-2545-4bc4-b65a-12bac1583248","cell_type":"markdown","source":"Now that the file is open, you can inspect the arrays we will be using.","metadata":{}},{"id":"930ab83a-9333-4d0b-86b6-f46ffef650e7","cell_type":"code","source":"import awkward as ak\nak.num(events.Electron, axis=1).compute()\nak.num(events.Muon, axis=-1).compute()\nak.num(events.Jet, axis=1).compute()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"44009313-3d8b-44ab-a75c-da6690bc8f0d","cell_type":"code","source":"events.Electron.fields\nevents.Muon.fields\nevents.Jet.fields","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"81139ea1-7b7b-49ca-8d39-48a1be44fbbc","cell_type":"markdown","source":"We can check the effect of applying quality criteria on the particle collections:","metadata":{}},{"id":"fc53b808-48ab-4b3a-8159-affcdd749003","cell_type":"code","source":"import numpy as np\nselected_electrons = events.Electron[(events.Electron.pt > 30) & (np.abs(events.Electron.eta) < 2.5) & (events.Electron.isTight == True)]\nselected_muons = events.Muon[(events.Muon.pt > 30) & (np.abs(events.Muon.eta) < 2.4) & (events.Muon.mediumId == True)]\nselected_jets = events.Jet[(events.Jet.pt > 25) & (np.abs(events.Jet.eta) < 2.4)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"091f5845-efa0-4bc6-8fd7-86bc28e16738","cell_type":"code","source":"events.Jet.pt[0].compute()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"75da6ad2-cc31-4530-ae45-b0b1e5f4990e","cell_type":"code","source":"selected_jets.pt[0].compute()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b7773683-73a3-49c1-af95-14bc66384388","cell_type":"markdown","source":"Next we will be looking at an event-level variable. After selecting the objects we require events to be selected by `event_filters` if they have at least four jets, two of them must be b-tagged. B-tagging discriminator scores (CSVV2) are stored in the root file per jet and higher values indicate higher probability that the jet originates from a b quark. In the CMS experiment we generally use a score threshold or working point which has 80% signal efficiency and 1% mis-tagging rate for light quarks. We want to then calculate the tri-jet invariant mass from the fully hadronic top decay: $t\\to bW^+ \\to bq\\bar{q}$. Note that the awakward array feature `combinations` makes it easy to implement the combinatorics involved in finding multiple tri-jet candidates which are then sorted based on b-tagging and $p_T$. Here is where the true power of columnar analysis and the wonderful python tools that are being developed become really evident. ","metadata":{}},{"id":"6ca193d1-8ff7-479a-842d-849ff408c2f7","cell_type":"code","source":"def calculate_trijet_mass(events):\n    # pT > 30 GeV for leptons, > 25 GeV for jets\n    selected_electrons = events.Electron[(events.Electron.pt > 30) & (np.abs(events.Electron.eta) < 2.5) & (events.Electron.isTight == True)]\n    selected_muons = events.Muon[(events.Muon.pt > 30) & (np.abs(events.Muon.eta) < 2.4) & (events.Muon.mediumId == True)]\n    selected_jets = events.Jet[(events.Jet.pt > 25) & (np.abs(events.Jet.eta) < 2.4)]\n\n    # single lepton requirement\n    event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n    # at least four jets\n    event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n    # at least two b-tagged jets (\"tag\" means score above threshold)\n    B_TAG_THRESHOLD = 0.8\n    event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n\n    # apply filters\n    selected_jets = selected_jets[event_filters]\n\n    trijet = ak.combinations(selected_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidate\n    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n\n    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n    # pick trijet candidate with largest pT and calculate mass of system\n    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n    return ak.flatten(trijet_mass)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4f1f8f4e-5975-49e2-aa95-b240579ad9c2","cell_type":"code","source":"calculate_trijet_mass(events).compute()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"46577b70-1c9f-4e06-b176-4b32d9878a00","cell_type":"markdown","source":"Coffea hist subpackage facilitates plotting by creating Hist objects. In physics analyses, we use histograms to visualize the data. We can’t plot every value of trijet mass, so we divide the range of masses into n bins across some reasonable range. Thus, we need to define the mapping for our reduction; defining the number of bins and the range is sufficient for this. This is called a Regular axis in the hist.Hist package, we are using 25 bins between values of 50 and 550 GeV.","metadata":{}},{"id":"549bda97-8f02-49ce-a0da-090689f2d77e","cell_type":"code","source":"import hist\nhistogram = hist.Hist.new.Reg(25, 50, 550, name=\"observable\", label=\"observable [GeV]\").StrCat([\"4j1b\", \"4j2b\"], name=\"region\", label=\"Region\").StrCat([], name=\"process\", label=\"Process\", growth=True).StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True).Weight()\nhistogram.fill(observable=calculate_trijet_mass(events).compute(), region=\"4j2b\", process=\"ttbar\", variation=\"nominal\", weight=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ba774edf-1491-4105-9e14-48ccb0a5054e","cell_type":"code","source":"import matplotlib.pyplot as plt\nhistogram[:,\"4j2b\",\"ttbar\",\"nominal\"].plot(histtype=\"fill\", linewidth=1, edgecolor=\"grey\", label='ttbar')\nplt.legend()\nplt.title(\">= 4 jets, >= 2 b-tags\")\nplt.xlabel(\"$m_{bjj}$ [Gev]\");\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b14f9bc5-12a2-41fa-b86c-4ee76fbeac15","cell_type":"markdown","source":"# Coffea executor\n\nWe would now like to scale this analysis up to far larger datasets for a practical analysis scenario. The first expansion we can do to our analysis is to consider running it over more datasets, which include all of our data, our background, and the signal. Additionally, we would like to show to how to estimate a few sources of systematic uncertainty, and for that we will be using, in some cases, additional datasets. These systematics datasets are generaly variations of the nominal ones.\n\nTo expand our analysis, we will use coffea Processors. Processors are coffea’s way of encapsulating an analysis in a way that is deployment-neutral. Once you have a Coffea analysis, you can throw it into a processor and use any of a variety of executors (e.g. Dask, Parsl, Spark) to chunk it up and run it across distributed workers. This makes scale-out simple and dynamic for users. Unfortunately, we don’t have the time to do such a demostration, but we will run it locally, with our vanilla coffea executor.","metadata":{}},{"id":"56047134-f08e-4068-b3b5-3f846ddd30f9","cell_type":"code","source":"from coffea.processor import ProcessorABC\nimport hist\nimport awkward as ak\nimport numpy as np\nclass TtbarAnalysis(ProcessorABC):\n    def __init__(self):\n        num_bins = 25\n        bin_low = 50\n        bin_high = 550\n        name = \"observable\"\n        label = \"observable [GeV]\"\n        #https://hist.readthedocs.io/en/latest/user-guide/quickstart.html\n        #StrCat = StrCategory\n        #https://hist.readthedocs.io/en/latest/banner_slides.html?highlight=StrCategory#many-axis-types\n        self.hist = (\n            hist.Hist.new.Reg(num_bins, bin_low, bin_high, name=name, label=label)\n            .StrCat([\"4j1b\", \"4j2b\"], name=\"region\", label=\"Region\")\n            .StrCat([], name=\"process\", label=\"Process\", growth=True)\n            .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n            .Weight()\n        )\n\n    def process(self, events):\n        hist_4j1b = self.hist.copy()\n        hist_4j2b = self.hist.copy()\n        hist_dict = {\"4j1b\": hist_4j1b, \"4j2b\": hist_4j2b}\n        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n        variation = events.metadata[\"variation\"]  # \"nominal\", \"scaledown\", etc.\n\n        # normalization for MC\n        x_sec = events.metadata[\"xsec\"]\n        nevts_total = events.metadata[\"nevts\"]\n        lumi = 2256.38\n        if process != \"data\":\n            xsec_weight = x_sec * lumi / nevts_total\n        else:\n            xsec_weight = 1\n\n        #object selection\n        selected_electrons = events.Electron[(events.Electron.pt > 30) & (abs(events.Electron.eta)<2.1) & (events.Electron.isTight == True)]\n        selected_muons = events.Muon[(events.Muon.pt > 30) & (abs(events.Muon.eta)<2.1) & (events.Muon.mediumId == True)]\n        selected_jets = events.Jet[(events.Jet.pt > 30) & (abs(events.Jet.eta)<2.4)]\n\n        # single lepton requirement\n        event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n\n        # at least four jets\n        event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n\n        # at least one b-tagged jet (\"tag\" means score above threshold)\n        B_TAG_THRESHOLD = 0.8\n        event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 >= B_TAG_THRESHOLD, axis=1) >= 1)\n\n        # apply event filters\n        selected_events = events[event_filters]\n        selected_electrons = selected_electrons[event_filters]\n        selected_muons = selected_muons[event_filters]\n        selected_jets = selected_jets[event_filters]\n\n        for region in [\"4j1b\", \"4j2b\"]:\n            # further filtering: 4j1b CR with single b-tag, 4j2b SR with two or more tags\n            if region == \"4j1b\":\n                region_filter = ak.sum(selected_jets.btagCSVV2 >= B_TAG_THRESHOLD, axis=1) == 1\n                selected_jets_region = selected_jets[region_filter]\n                # use HT (scalar sum of jet pT) as observable\n                observable = ak.sum(selected_jets_region.pt, axis=-1).compute()\n                hist_4j1b.fill(observable=observable, region=region, process=process, variation=variation, weight=xsec_weight)\n\n            elif region == \"4j2b\":\n                region_filter = ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2\n                selected_jets_region = selected_jets[region_filter]\n\n                # reconstruct hadronic top as bjj system with largest pT\n                # the jet energy scale / resolution effect is not propagated to this observable at the moment\n                trijet = ak.combinations(selected_jets_region, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n                trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # calculate four-momentum of tri-jet system\n                trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n                trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # require at least one-btag in trijet candidates\n                # pick trijet candidate with largest pT and calculate mass of system\n                trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n                observable = ak.flatten(trijet_mass).compute()\n                hist_4j2b.fill(observable=observable, region=region, process=process, variation=variation, weight=xsec_weight)\n\n        output = {\"hist\": hist_dict}\n\n        return output\n\n    def postprocess(self, accumulator):\n        pass\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4ee8c505-8931-472b-98c5-ef06019dd93f","cell_type":"markdown","source":"The datasets over which we are running are still quite large. So running on the whole dataset in a single computer is not very efficient. Here is where coffea really performs, because you can ship it to different worker nodes using some different executors. In this example we will use `dask` but on a single file.","metadata":{}},{"id":"fe84708e-1eb1-4107-844a-714e1fb990a9","cell_type":"code","source":"from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n\nevents = NanoEventsFactory.from_root(\n    {\"https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0000.root\": \"Events\"},\n    metadata={\"process\": \"ttbar\", \"variation\" : \"nominal\", \"xsec\" : 833, \"nevts\" : 1334428},\n    schemaclass=NanoAODSchema,\n).events()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c5243646-95dd-4dcf-9e59-b186c7d8fdce","cell_type":"code","source":"import dask\np = TtbarAnalysis()\nout = p.process(events)\n(computed,) = dask.compute(out)\nprint(computed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9683fba1-9bb3-4b63-a4c4-24bc3d65b3e2","cell_type":"markdown","source":"You can add the control region histogram here for visualization.","metadata":{}},{"id":"acdaa203-92a2-4db8-a98c-a0792c536824","cell_type":"code","source":"import matplotlib.pyplot as plt\n\nhist_4j1b = computed['hist']['4j1b']\nprint(hist_4j1b)\n\nhist_4j1b_nominal = hist_4j1b[\n    dict(region='4j1b', process='ttbar', variation='nominal')\n]\n\nfig, ax = plt.subplots()\nhist_4j1b_nominal.plot1d(ax=ax)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"441915fb-8c02-4b05-b168-73f4d828dfc0","cell_type":"markdown","source":"# Running over a list of samples\n\nIn an analysis we will have a list of files we need to analyze, for this purpose we will use the json format. Inspect the `nanoaod_inputs.json` file content which lists several samples along with some metadata. We want better statistics of the Monte Carlo samples to reduce uncertainty from statistical fluctuations. In addition, we use these samples for background nomalization, various efficiency measurements, kinematics studies, machine-learning model training etc. Now we will be using 20 files from the ttbar sample to obatin the same histograms. ","metadata":{}},{"id":"decbfe82-5559-4413-8f9f-1b735b6fcc8d","cell_type":"code","source":"import json\nwith open(\"nanoaod_inputs.json\") as f:\n    file_info = json.load(f)\n\nfiles = {}\n\ntot_events = file_info[\"ttbar\"][\"nominal\"][\"nevts_total\"]\nfile_list = file_info[\"ttbar\"][\"nominal\"][\"files\"]\nfile_paths = [f[\"path\"] for f in file_list]\n\nnevts_total = sum([f[\"nevts\"] for f in file_list[:20]])\n\n# Create { \"path/to/file.root\": \"Events\" } for each file\nfiles_dict = {path: \"Events\" for path in file_paths[:20]}\n\nprint(len(files_dict))\nprint(nevts_total)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"306d3a82-0bb6-4ae6-b2a7-f5408a9dc852","cell_type":"code","source":"events_nom = NanoEventsFactory.from_root(\n    files_dict,\n    metadata={\"process\": \"ttbar\", \"variation\" : \"nominal\", \"xsec\" : 833, \"nevts\" : nevts_total},\n    schemaclass=NanoAODSchema,\n).events()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"518ce9fb-0b3f-44d7-860f-3786f6083894","cell_type":"code","source":"import time\n\nt0 = time.monotonic()\nout_nom = p.process(events_nom)\n(computed_nom,) = dask.compute(out_nom)\nexec_time = time.monotonic() - t0\n\nprint(computed_nom)\nprint(f\"\\nexecution took {exec_time:.2f} seconds\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9f156aab-12fe-4b86-8e80-2e8b7daea910","cell_type":"markdown","source":"We are roughly analyzing 130k events per second. From here you can add the functionality to process files from each sample in the json and create a collection of histograms for further analaysis. [Here](https://cernbox.cern.ch/index.php/s/SPcoOLArZCFFupN) you can download a pre-made `histograms.root` file. It contains all the histograms produced by the analysis processor and was obtained after 4 hours of running on a single computer. Of course, if you have some available space, you could download the files (the size is not terribly large, maybe around 60GB or so), and run much faster without replying on remote server. Use the provided plotting script to visualize the results: ","metadata":{}},{"id":"e9a4e919-6214-4f81-81a8-850bebf42f5a","cell_type":"code","source":"import uproot\nimport hist\nimport mplhep as hep\nimport matplotlib.pyplot as plt\n\nfile = uproot.open(\"histograms.root\")\nfile.classnames()\ndata = file['4j2b_data'].to_hist()\nttbar = file['4j2b_ttbar'].to_hist()\nsingle_atop = file['4j2b_single_atop_t_chan'].to_hist()\nsingle_top = file['4j2b_single_top_t_chan'].to_hist()\nsingle_tW = file['4j2b_single_top_tW'].to_hist()\nwjets = file['4j2b_wjets'].to_hist()\nbklabels = [\"ttbar\",\"sigle_atop\",\"single_top\",\"single_tW\",\"wjets\"]\n\nhep.style.use(\"CMS\")\nhep.cms.label(\"open data\",data=True, lumi=2.26, year=2015)\nhep.histplot(data,histtype=\"errorbar\", color='k', capsize=4, label=\"Data\")\nhep.histplot([ttbar,single_atop,single_top,single_tW,wjets],stack=True, histtype='fill', label=bklabels, sort='yield')\nplt.legend(frameon=False)\nplt.xlabel(\"$m_{bjj}$ [Gev]\");\nplt.savefig('finalplot.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}